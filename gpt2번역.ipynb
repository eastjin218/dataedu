{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/8b/02/c1260ff4caf483c01ce36ca45a63f05417f732d94ec42cce292355dc7ea4/tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 24.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (2.23.0)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (3.14.0)\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (0.11.0)\n",
      "Collecting promise\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (4.51.0)\n",
      "Collecting dill\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/52/d6/79f40d230895fa1ce3b6af0d22e0ac79c65175dc069c194b79cc8e05a033/dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 102.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (3.7.4.3)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/57/95/daf3259ed1ca4f262bde2c483356174c86b6a6a39e5dbf814801c52b7b8d/tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 76.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/c5/1f/ec86d2a5c48ac6490d4471b297885603cf0e8da89d5ffbf0bce6e57f4d64/importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: termcolor in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (1.19.4)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (18.1.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow_datasets) (0.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/03/74/3956721ea1eb4bcf7502a311fdaa60b85bd751de4e57d1943afe9b334141/googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 105.9 MB/s \n",
      "\u001b[?25hCollecting zipp>=0.4; python_version < \"3.8\"\n",
      "  Downloading http://mirror.kakao.com/pypi/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=13ef29cfc2087ab0cbf606c764f89954f3d22907854bed53aed8f443ed0900bd\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/cb/32/4f/8ad99d466e2fe5a68d47b0db9aeeb5b04c62a09f50becdbc92\n",
      "Successfully built promise\n",
      "Installing collected packages: promise, dill, googleapis-common-protos, tensorflow-metadata, zipp, importlib-resources, tensorflow-datasets\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow-metadata 0.25.0 requires absl-py<0.11,>=0.9, but you'll have absl-py 0.11.0 which is incompatible.\u001b[0m\n",
      "Successfully installed dill-0.3.3 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 zipp-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset ted_hrlr_translate/pt_to_en/1.0.0 (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /home/ubuntu/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb59d188e1c43e3aee7104d3f793acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72921f18bc154384b794aad3982b6708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Dl Size...'), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e15c6a18319419aaa8c7a179fa937f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Extraction completed...'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23da52bf890b40b79aec20e76abfe077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e57414c48f481dbcb909fb9316d0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=51785.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ubuntu/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteWTH64F/ted_hrlr_translate-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2903b86ab54f68ad8f2f20273373c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=51785.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f760d6cc88a643c4aa1fc4e260afe74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1193.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ubuntu/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteWTH64F/ted_hrlr_translate-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15f5cc853784e1aad56e25d933f1554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1193.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df8d490bc894fbb9f8f4f8108341c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1803.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ubuntu/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteWTH64F/ted_hrlr_translate-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb5b7cef4354994a2eee2a4dc345a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1803.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /home/ubuntu/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples, test_examples = examples['train'], examples['validation'], examples['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = [x.numpy().decode() for x,y in train_examples]\n",
    "eng = [y.numpy().decode() for x,y in train_examples]\n",
    "pt_val = [x.numpy().decode() for x,y in val_examples]\n",
    "eng_val = [y.numpy().decode() for x,y in val_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pt + eng + pt_val + eng_val\n",
    "f = open(\"train.txt\", 'w')\n",
    "for i in range(len(train_data)):\n",
    "    data =train_data[i] + \"\\n\"\n",
    "    f.write(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .',\n",
       " 'mas e se estes fatores fossem ativos ?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pt + eng + pt_val + eng_val\n",
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    }
   ],
   "source": [
    "print(pt[0])\n",
    "print(pt[1])\n",
    "print(eng[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://ftp.daumkakao.com/pypi/simple\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.1.91)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=train.txt --pad_id=0 --bos_id=2 --eos_id=3 --unk_id=1 --model_prefix=gpt2_spm --vocab_size=40000 --character_coverage=1.0 --model_type=bpe\n"
     ]
    }
   ],
   "source": [
    "# sentencepiece를 활용하기\n",
    "templates = '--input={} --pad_id={} --bos_id={} --eos_id={} --unk_id={} --model_prefix={} --vocab_size={} --character_coverage={} --model_type={}'\n",
    "\n",
    "\n",
    "train_input_file = 'train.txt' # train.txt에서 문장 불러 오기\n",
    "pad_id = 0\n",
    "vocab_size = 40000 # 최대 단어 수는 40000으로 한다\n",
    "prefix = \"gpt2_spm\" # sentencepiece가 gpt2_spm.model gpt2_spm.vocab으로 저장된다.\n",
    "unk_id = 1 # 모르는 단어 [UNK]의 인덱스\n",
    "bos_id = 2 # Beginning of sentence, 문장의 시작을 알려주는 인덱스\n",
    "eos_id = 3 # End of sentecnce, 문장의 끝을 알려주는 인덱스\n",
    "character_coverage = 1.0 # 한국어는 0.9정도가 좋음.(단어 하나가 함의하는게 클수록 작게 준다)\n",
    "model_type = 'bpe' # 토크나이징에 bpe 기법을 사용 예정\n",
    "\n",
    "cmd = templates.format(train_input_file,\n",
    "                       pad_id,\n",
    "                       bos_id,\n",
    "                       eos_id,\n",
    "                       unk_id,\n",
    "                       prefix,\n",
    "                       vocab_size,\n",
    "                       character_coverage,\n",
    "                       model_type)\n",
    "print(cmd)\n",
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd) # sentencepiece 모델을 훈련시킨다\n",
    "\n",
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.Load(\"gpt2_spm.model\") # 훈련한 모델을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt2_spm.vocab\", encoding='utf-8') as f:\n",
    "    Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
    "word2idx = {w[0] : i for i, w in enumerate(Vo)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어 수 40000\n",
      "포르투갈어 e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "영어 and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "포르투갈어 인덱스 부여 [8, 347, 883, 168, 4, 3646, 6, 13430, 4, 2292, 9068, 138, 7232, 6, 56, 89, 4, 32173, 260, 7]\n",
      "인덱스를 토큰으로(포르투갈어) ['▁e', '▁quando', '▁melhor', 'amos', '▁a', '▁procura', '▁,', '▁tiramos', '▁a', '▁única', '▁vantagem', '▁da', '▁impressão', '▁,', '▁que', '▁é', '▁a', '▁serendip', 'idade', '▁.']\n",
      "인덱스를 토큰으로(영어) ['▁and', '▁when', '▁you', '▁improve', '▁search', 'ability', '▁,', '▁you', '▁actually', '▁take', '▁away', '▁the', '▁one', '▁advantage', '▁of', '▁print', '▁,', '▁which', '▁is', '▁serendipity', '▁.']\n",
      "토큰을 원래 문장으로 and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "시작단어 <s> 끝단어 </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"총 단어 수\", sp.get_piece_size())\n",
    "# 토큰화\n",
    "print(\"포르투갈어\", pt[0])\n",
    "print(\"영어\", eng[0])\n",
    "print(\"포르투갈어 인덱스 부여\", sp.EncodeAsIds(pt[0]))\n",
    "print(\"인덱스를 토큰으로(포르투갈어)\", sp.EncodeAsPieces(pt[0]))\n",
    "print(\"인덱스를 토큰으로(영어)\", sp.EncodeAsPieces(eng[0]))\n",
    "#토큰 -> 문자열\n",
    "print(\"토큰을 원래 문장으로\", sp.DecodePieces(sp.EncodeAsPieces(eng[0])))\n",
    "print(\"시작단어\",sp.id_to_piece(2), \"끝단어\",sp.id_to_piece(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sp.piece_to_id(\"<s>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "  # 포르투갈어 + <영어 시작> + 영어 + <영어 끝>\n",
    "    lang = sp.EncodeAsIds(lang1) + [sp.piece_to_id(\"<s>\")] + sp.EncodeAsIds(lang2) + [sp.piece_to_id(\"</s>\")]# 포르투갈 어를 인코딩 할 때 시작 단어를 의미하는 숫자와, 끝 단어를 의미하는 숫자가 붙음\n",
    "  # 텐서플로우에서 활용할 수 있는 텐서로 바꿔준다\n",
    "    lang = tf.convert_to_tensor(lang)\n",
    "  # 텐서의 모양을 정해줘야 함, 문장의 길이는 변하므로 [None]으로\n",
    "    lang.set_shape([None])\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = 트레인 데이터 셋\n",
    "# texts_val = valid 데이터 셋\n",
    "texts = [encode(x,y) for x,y in zip(pt, eng)]\n",
    "texts_val = [encode(x,y) for x,y in zip(pt_val, eng_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 만약 문장이 인코딩 되었을 때, 인코딩 된 길이가 100을 초과하면 데이터에서 배제하고자 하는 필터 함수임\n",
    "def filter_max_length(x, max_length=MAX_LENGTH):\n",
    "    return tf.size(x) <= max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[    8   347   883   168     4  3646     6 13430     4  2292  9068   138\n",
      "  7232     6    56    89     4 32173   260     7     2    58   358    90\n",
      "  5557  4278  3129     6    90   563   777  1636    32   277  8868    64\n",
      "  5182     6   534    80 39049     7     3], shape=(43,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.3.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages\n",
      "Requires: scipy, tensorflow-estimator, gast, wrapt, absl-py, keras-preprocessing, opt-einsum, tensorboard, h5py, google-pasta, termcolor, six, numpy, wheel, grpcio, protobuf, astunparse\n",
      "Required-by: tensorflow-serving-api\n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "<function generator at 0x7f08d30ad510>\n"
     ]
    }
   ],
   "source": [
    "print(type(generator))\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([    8,   347,   883,   168,     4,  3646,     6, 13430,     4,\n",
      "        2292,  9068,   138,  7232,     6,    56,    89,     4, 32173,\n",
      "         260,     7,     2,    58,   358,    90,  5557,  4278,  3129,\n",
      "           6,    90,   563,   777,  1636,    32,   277,  8868,    64,\n",
      "        5182,     6,   534,    80, 39049,     7,     3]), array([  174,     8,    96,   981,  9414,  5329, 12064,   121,     2,\n",
      "         180,   190,   306,    84,   439,  7192,   121,     3]), array([  174,   524,   126,  1882,     4,  9094,    39,   144,  6975,\n",
      "           7,     2,   180,   169,   547,    23, 39911, 39892,  1231,\n",
      "         111,  9071,     7,     3])]\n"
     ]
    }
   ],
   "source": [
    "# Train 데이터(texts)를 generator를 활용하여 텐서플로우 dataset을 만듦\n",
    "def generator():\n",
    "    for el in texts:\n",
    "        yield el\n",
    "dataset = tf.data.Dataset.from_generator(generator,\n",
    "                                         output_types= tf.int64,output_shapes=tf.TensorShape([None,]))\n",
    "dataset = dataset.filter(filter_max_length) # 길이가 100 초과하면 날리기\n",
    "dataset = dataset.cache() # cache를 활용해서 데이터를 로드할 때 빠른 처리를 기대해 봄\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=[None]) # padded_batch 불러오기\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) # 데이터가 훈련되는 사이에, 데이터를 미리 준비해 놓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid 데이터 또한 텐서플로우 dataset으로 만들기\n",
    "def val_generator():\n",
    "    for el in texts_val:\n",
    "        yield el\n",
    "val_dataset = tf.data.Dataset.from_generator(val_generator,\n",
    "                                           output_types= tf.int64)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE, padded_shapes=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 임베딩 정의\n",
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, initializer=None, stddev=0.02, mean=0.0):\n",
    "        super().__init__(self)\n",
    "        self.vocab_size = vocab_size # 훈련할 데이터의 vocab size\n",
    "        self.embedding_size = embedding_size # 단어가 고차원으로 임베딩되는 차원\n",
    "        self.stddev = stddev # 초기 임베딩 계수들의 표준편차\n",
    "        self.mean = mean # 초기 임베딩 계수들의 평균\n",
    "        self.initializer = initializer\n",
    "        if self.initializer is None:\n",
    "            self.initializer = tf.random_normal_initializer(mean=self.mean,\n",
    "                                                      stddev = self.stddev) # N~(0, 2^2)의 분포로 초기 계수를 정규화\n",
    "    def build(self, input_shape):\n",
    "        with tf.name_scope(\"embedding_weights\"):\n",
    "            self.embedding_weights = self.add_weight(\n",
    "              \"weights\",\n",
    "              shape=[self.vocab_size, self.embedding_size],\n",
    "              dtype=\"float32\",\n",
    "              initializer=self.initializer) # [vocab_size, embedding_size]의 계수 정의\n",
    "  \n",
    "    def call(self, inputs, mode=\"embedding\", scale=False):\n",
    "        if mode == \"embedding\":\n",
    "            return self.embedding(inputs, scale=scale) # embedding 모드이면, 인풋 단어를 임베딩 할 때 사용한다\n",
    "        elif mode == \"projection\":\n",
    "            return self.projection(inputs) # projection 모드이면, 최종 출력 값에 임베딩 계수들을 다시 곱해준다\n",
    "        else:\n",
    "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
    "  \n",
    "    def embedding(self, inputs, scale=False):\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            mask = tf.cast(tf.not_equal(inputs, 0), tf.float32) # 마스크를 만들어서 패딩인 부분을 0으로 마스크\n",
    "            inputs = tf.cast(inputs, tf.int32) # 인풋 값을 텐서로 바꿔줌, input_shape -> [batch_size, length]\n",
    "\n",
    "        embeddings = tf.nn.embedding_lookup(self.embedding_weights, inputs) # 임베딩 룩업 테이블을 곱해서 단어를 임베딩\n",
    "      # embeddings_shape -> [batch_size, length, embedding_size]\n",
    "      # mask size is converted to [batch_size, length, 1]\n",
    "        embeddings *= tf.expand_dims(mask, -1)\n",
    "\n",
    "        if scale:\n",
    "            embeddings *= self.embedding_size ** 0.5\n",
    "\n",
    "        return embeddings\n",
    "  \n",
    "  # GPT2 최종 출력 값에, 임베딩 벡터를 다시 곱해줌\n",
    "    def projection(self, inputs):\n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_len = tf.shape(inputs)[1]\n",
    "      # 출력 값의 input shape [batch_size, seq_len, d_model] 이다\n",
    "      # seq_len : 문장 길이, d_model = 단어 embedding 차원\n",
    "            h_flat = tf.reshape(inputs, [-1, self.embedding_size])\n",
    "      # h_flat shape = [batch_size*seq_len, d_model]\n",
    "            logits = tf.matmul(h_flat, self.embedding_weights, transpose_b=True)\n",
    "      # logits shape = [batch_size*seq_len, vocab_size]\n",
    "            return tf.reshape(logits, [batch_size, seq_len, self.vocab_size])\n",
    "      # final output shape = [batch_size, seq_len, vocab_size]\n",
    "\n",
    "# 포지셔널 임베딩 정의\n",
    "class PositionEmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position_seq, pos_embedding_size, trainable=True, stddev=0.02, mean=0.0):\n",
    "        tf.keras.layers.Layer.__init__(self, name=\"PositionEmbeddingLayer\")\n",
    "\n",
    "        self.position_seq = position_seq # 보통 적당히 큰 숫자(10000) 정도로 입력한다음 문장의 길이에 맞게 자름\n",
    "        self.hidden_size = pos_embedding_size\n",
    "        self.trainable = trainable\n",
    "        self.stddev = stddev\n",
    "        self.mean = mean\n",
    "\n",
    "        if trainable:\n",
    "            self.position_embedding = EmbeddingLayer(self.position_seq, self.hidden_size, stddev=self.stddev, mean=self.mean)\n",
    "\n",
    "    def call(self, inputs, start=1):\n",
    "        with tf.name_scope(\"pos_embedding\"):\n",
    "            if self.trainable:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                batch_seq = tf.shape(inputs)[1]\n",
    "\n",
    "                positions = tf.reshape(tf.tile(tf.range(start, batch_seq + start), [batch_size]), [batch_size, batch_seq])\n",
    "                # positions는 직접 해보면 좋음\n",
    "                positions = tf.cast(positions, tf.int32)\n",
    "                position_mask = tf.cast(tf.not_equal(inputs, 0), tf.int32)\n",
    "                positions *= position_mask\n",
    "\n",
    "                return self.position_embedding(positions)\n",
    "\n",
    "            else:\n",
    "                return self.get_position_sinusoid(self.position_seq, self.hidden_size) # 트랜스포머에서 한 것과 유사함. 생략해도 좋음\n",
    "\n",
    "    @staticmethod\n",
    "    def get_position_sinusoid(seq_len, hidden_size, min_timescale=1.0, max_timescale=1.0e4):\n",
    "        position = tf.cast(tf.range(seq_len), tf.float32)\n",
    "        num_timescales = hidden_size // 2\n",
    "        log_timescale_increment = (math.log(float(max_timescale) / float(min_timescale)) / (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(tf.cast(tf.range(num_timescales), tf.float32) * -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, att_dropout=0.1, residual_dropout=0.1, scale=True):\n",
    "        super().__init__(self, name=\"MultiHeadAttention\")\n",
    "        self.num_heads = num_heads # 병렬로 나눠서 처리할 숫자\n",
    "        self.d_model = d_model # 임베딩 차원\n",
    "        self.att_dropout = att_dropout # 어텐션 드롭아웃\n",
    "        self.residual_dropout = residual_dropout # 레지듀얼 드롭아웃\n",
    "        self.scale = scale # 스케일링 할지 말지 (Query * Value의 값을 SQRT(임베딩차원) 으로 나눔)\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.c_attn = tf.keras.layers.Dense(self.d_model * 3) # 출력 차원을 3배로 한 다음에 q, k, v로 나눔 \n",
    "        self.c_proj = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "\n",
    "    def multihead_attention(self, q, k, v, training, mask=None):\n",
    "    # depth : d_model // seq_len\n",
    "    # q_shape : [batch_size, num_heads, seq_q, depth]\n",
    "    # k_shape : [batch_size, num_heads, seq_k, depth]\n",
    "    # v_shape : [batch_size, num_heads, seq_v, depth]\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True) # shape -> [batch_size, num_heads, seq_q, seq_k]\n",
    "        if self.scale:\n",
    "            dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "            matmul_qk = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            matmul_qk += (mask * -1e9) # 마스크를 곱해준다\n",
    "\n",
    "        attention_weights = tf.nn.softmax(matmul_qk, axis=-1) # shape -> [batch_size, num_heads, seq_q, seq_k]\n",
    "\n",
    "        if training:\n",
    "            attention_weights = tf.nn.dropout(attention_weights, rate=self.att_dropout, name=\"attn_dropout\")\n",
    "        output = tf.matmul(attention_weights, v) # output shape -> [batch_size, num_heads, seq_q, depth]\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # x shape -> [batch_size, num_heads, seq_q, depth]\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        # x shape -> [batch_size, seq_q, num_heads, depth]\n",
    "        merged = tf.reshape(x, (batch_size, -1, self.d_model))\n",
    "        # merged shape -> [batch_size, seq_q, d_model]\n",
    "        return merged\n",
    "\n",
    "    def call(self, x, mask=None, past_layer=None, training=True):\n",
    "        # input x shape -> [batch_size, seq_len, d_model]\n",
    "        x = self.c_attn(x)\n",
    "        # x shape -> [batch_size, seq_len, d_model * 3]\n",
    "        query, key, value = tf.split(x, 3, axis=2)\n",
    "        # query_shape -> [batch_size, seq_len, d_model]\n",
    "        # key_shape -> [batch_size, seq_len, d_model]\n",
    "        # value_shape -> [batch_size, seq_len, d_model]\n",
    "\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "\n",
    "        # query_shape -> [batch_size, num_heads, seq_len, d_model // num_heads]\n",
    "        # key_shape -> [batch_size, num_heads, seq_len, d_model // num_heads]\n",
    "        # value_shape -> [batch_size, num_heads, seq_len, d_model // num_heads]\n",
    "\n",
    "        # past -> 과거 자료 활용\n",
    "        # 예를 들어 \"I have my chair repaired to study NLP\"라는 문장이 있다고 하자\n",
    "        # 마지막 단어를 예측할 때 NLP라는 단어를 예측한다고 하면\n",
    "        # I have my chair repaired to study라는 문장을 GPT에 넣으면 NLP를 예측하게 된다.\n",
    "        # 그런데 i have my chair repaired to 를 활용해서 study 라는 단어도 예측했을 것이다.\n",
    "        # 따라서 i have my chair repaired to 를 GPT에 넣었을 때의 출력을 재활용하면\n",
    "        # study라는 단어만 신경망을 거치면 되므로 예측 속도가 상당히 빨라지게 된다.\n",
    "\n",
    "        if past_layer is not None:\n",
    "          # past_layer의 shape은 [batch_size, 2, num_heads, seq_past_len, d_model // num_heads]\n",
    "            past_key, past_value = tf.unstack(past_layer, axis=1)\n",
    "          # past_key_shape = [batch_size, num_heads, seq_past_len, d_model // num_heads]\n",
    "          # past_value_shape = [batch_size, num_heads, seq_past_len, d_model // num_heads]\n",
    "            key = tf.concat([past_key, key], axis=-2)\n",
    "\n",
    "            value = tf.concat([past_value, value], axis=-2)\n",
    "          # key_value_shape = [batch_size, num_heads, seq_past_len + seq_len, d_model // num_heads]\n",
    "          # value_value_shape = [batch_size, num_heads, seq_past_len + seq_len, d_model // num_heads]\n",
    "        present = tf.stack([key, value], axis=1)\n",
    "        # present shape -> [batch_size, 2, num_heads, seq_past_len + seq_len, d_model // num_heads]\n",
    "        scaled_attention, attention_weights = self.multihead_attention(query, key, value, training, mask)\n",
    "        #scaled_attention shape -> [batch_size, num_heads, seq_len, d_model // num_heads]\n",
    "        #attention_weights_shape -> [batch_size, num_heads, seq_len, seq_past_len + seq_len, d_model // num_heads]\n",
    "        concat_attention = self.merge_heads(scaled_attention)\n",
    "        # concat_attention shape -> [batch_size, seq_len, d_model]\n",
    "        output = self.c_proj(concat_attention)\n",
    "\n",
    "        if training:\n",
    "            output = tf.nn.dropout(output, rate=self.residual_dropout, name=\"resid_dropout\")\n",
    "        # output shape -> [batch_size, seq_len, d_model]\n",
    "        return output, present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, dff, dropout_rate=0.1, activation=tf.nn.relu):\n",
    "        super().__init__(self, name=\"FeedForward\")\n",
    "        self.hidden_size = hidden_size # d_model\n",
    "        self.dff = dff # middle dimension\n",
    "        self.activation = activation # relu \n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dense_layer = tf.keras.layers.Dense(self.dff)\n",
    "        self.output_dense_layer = tf.keras.layers.Dense(self.hidden_size)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # input x shape -> [batch_size, seq_len, d_model]\n",
    "        output = self.dense_layer(x)\n",
    "        # shape -> [batch_size, seq_len, dff]\n",
    "        output = self.activation(output)\n",
    "        output = self.output_dense_layer(output)\n",
    "        # shape -> [batch_size, seq_len, d_model]\n",
    "        if training:\n",
    "            output = tf.nn.dropout(output, rate=self.dropout_rate, name=\"feed_forward_dropout\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, i=0, dropout_rate = 0.1):\n",
    "        super().__init__(self, name=\"Decode_{}\".format(i))\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.d_model, self.num_heads, self.dropout_rate)\n",
    "        self.feed_forward = FeedForward(self.d_model, self.dff, self.dropout_rate)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training, mask, past=None):\n",
    "        # input x -> [batch_size, seq_len, d_model]\n",
    "        out, present = self.mha(self.layer_norm1(x), mask=mask, past_layer=past, training=training)\n",
    "        # out -> [batch_size, seq_len, d_model]\n",
    "        # present shape -> [batch_size, 2, num_heads, seq_past_len + seq_len, d_model // num_heads]\n",
    "        with tf.name_scope(\"residual_conn_1\"):\n",
    "            x = x + out\n",
    "\n",
    "        out = self.feed_forward(self.layer_norm2(x), training=training)\n",
    "        with tf.name_scope(\"residual_conn_2\"):\n",
    "            x = x + out\n",
    "        return x, present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, proj_weights=None, kernel_initializer=None):\n",
    "        super().__init__(self, name=\"OutputLayer\")\n",
    "        self.proj_weights = proj_weights\n",
    "        self.output_dim = output_dim\n",
    "        self.layer_weights = None\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.layer_weights = tf.keras.layers.Dense(self.output_dim)\n",
    "    def call(self, x):\n",
    "    # input x shape -> [batch_size, seq_len, d_model]\n",
    "        batch, sequence, d_model = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[-1]\n",
    "\n",
    "        if self.proj_weights is None:\n",
    "\n",
    "            out = self.layer_weights(x)\n",
    "          # out shape -> [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            out = self.proj_weights(x)\n",
    "          # out shape -> [batch_size, seq_len, vocab_size]\n",
    "        out = tf.reshape(out, [batch, sequence, self.output_dim])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수 정의\n",
    "def shape_as_list_2(x):\n",
    "    return [int(i) for i in tf.shape(x)]\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    with tf.name_scope(\"gelu\"):\n",
    "        cdf = 0.5 * (1.0 + tf.tanh(\n",
    "            (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "        return x * cdf\n",
    "\n",
    "\n",
    "def get_padding_mask(seq):\n",
    "    with tf.name_scope(\"Padding_Mask\"):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def attention_mask(size):\n",
    "    \"\"\"\n",
    "    if size is 4 then it returns below matrix\n",
    "       [[0., 1., 1., 1.],\n",
    "        [0., 0., 1., 1.],\n",
    "        [0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0.]]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"attention_mask\"):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def create_masks(inp):\n",
    "    with tf.name_scope(\"att_masking\"):\n",
    "        att_mask = attention_mask(tf.shape(inp)[1])\n",
    "        padding_mask = get_padding_mask(inp)\n",
    "        mask = tf.maximum(padding_mask, att_mask)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, max_seq_len, vocab_size,\\\n",
    "               rev_embedding_projection=True):\n",
    "        super().__init__(self, name=\"GPT2_TOTAL\")\n",
    "\n",
    "        self.rev_embedding_projection = rev_embedding_projection # 임베딩과 출력증을 묶을지\n",
    "        self.num_layers = num_layers # 디코더 층\n",
    "        self.num_heads = num_heads # 멀티 헤드 어텐션 때 헤드 숫자\n",
    "        self.dff = dff # 중간층 차원\n",
    "        self.max_seq_len = max_seq_len # position embedding시 임베딩할 최대 넓이\n",
    "        self.vocab_size = vocab_size # 단어의 수\n",
    "        self.d_model = d_model # 단어가 임베딩 될 때 차원\n",
    "\n",
    "        self.embedding = EmbeddingLayer(self.vocab_size, self.d_model) # 단어 임베딩\n",
    "        self.pos_embedding = PositionEmbeddingLayer(self.max_seq_len, self.d_model) # 위치 임베딩\n",
    "        self.decoder_layers = [DecoderLayer(self.d_model, self.num_heads, self.dff, i) for i in range(self.num_layers)] # 디코더를 num_layers 만큼 쌓음\n",
    "\n",
    "        if not self.rev_embedding_projection: # 만약 임베딩과 출력증을 같이 묶지 않으면\n",
    "            self.output_layer = OutputLayer(self.vocab_size)\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, training=True, past=None):\n",
    "    \n",
    "        x = tf.cast(x, tf.int32) # [batch_size, seq_len]\n",
    "        batch, sequence = tf.shape(x)[0], tf.shape(x)[1]\n",
    "\n",
    "        if past is None:\n",
    "            pasts = [None] * self.num_layers\n",
    "        else:\n",
    "            pasts = past\n",
    "    \n",
    "        assert len(pasts) == self.num_layers\n",
    "\n",
    "        att_mask = create_masks(x) # 어텐션 마스크\n",
    "\n",
    "        past_length = 1 if past is None else tf.shape(past)[-2]\n",
    "\n",
    "        with tf.name_scope(\"embeddings\"):\n",
    "            embedded_x = self.embedding(x)\n",
    "            hidden_states = embedded_x + self.pos_embedding(x, start=past_length) # hidden_states -> [batch_size, seq_len, d_model]\n",
    "    \n",
    "        presents = []\n",
    "        for decoder_layer, past in zip(self.decoder_layers, pasts):\n",
    "            hidden_states, present = decoder_layer(hidden_states, training, att_mask, past=past)\n",
    "      # hidden_states -> [batch_size, seq_len, d_model]\n",
    "      # present shape -> [batch_size, 2, num_heads, seq_past_len + seq_len, d_model // num_heads]\n",
    "            presents.append(present)\n",
    "\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "\n",
    "        if self.rev_embedding_projection: # 만약 임베딩과 출력층을 결합한다면\n",
    "            logits = self.embedding(hidden_states, mode=\"projection\")\n",
    "          # shape -> [batch_size, seq_len, vocab_size]\n",
    "        else:\n",
    "            logits = self.output_layer(hidden_states)\n",
    "          # shape -> [batch_size, seq_len, vocab_size]\n",
    "        return logits, presents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4 # 디코더 층\n",
    "d_model = 256 # 단어 임베딩 차원\n",
    "num_heads = 4 # 어텐션 크기\n",
    "dff = 512 # 중간 차원\n",
    "max_seq_len = 10000 # 위치 임베딩의 길이\n",
    "vocab_size = 40000 # 어휘 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') #from_logits=True로 하면 Dense 이후 softmax layer 값 출력\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # 예를 들어서 실제 자료(0은 패딩)가 [1,2,3,4,5,0,0,0,0,0] 이라면 [0,0,0,0,0,1,1,1,1,1]로 바꿔 줌\n",
    "                                                     # 이후 tf.math.logical_not을 활용해서 [True,True,True,True,True,False,False,False,False,False]으로 바꿔 줌\n",
    "    loss_ = loss_object(real, pred) # loss_는 패딩을 고려하지 않은 loss 값\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype) # [True,True,True,True,True,False,False,False,False,False]를 [1,1,1,1,1,0,0,0,0,0] 으로 바꿔 줌\n",
    "    loss_ *= mask # loss에 mask를 곱해서, 패딩인 부분은 0처리 해줌\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "# 패딩이 아닌 부분만을 가지고 정확도를 측정함\n",
    "def get_padded_accuracy(labels, logits):\n",
    "    with tf.name_scope(\"padded_accuracy\"):\n",
    "        weights = tf.cast(tf.not_equal(labels, 0), tf.float32)\n",
    "\n",
    "        outputs = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        padded_labels = tf.cast(labels, tf.int32)\n",
    "\n",
    "        nonpad_seq = tf.math.count_nonzero(weights, dtype=tf.dtypes.float32, )\n",
    "        acc = tf.cast(tf.equal(outputs, padded_labels), tf.float32)\n",
    "\n",
    "        accuracy = tf.reduce_sum(tf.cast(acc * weights, tf.float32)) / nonpad_seq\n",
    "\n",
    "    return tf.cast(accuracy, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 체크포인트 지정\n",
    "checkpoint_path = \"./\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(gpt2=gpt2,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93320173 0.7463982  0.94402003 0.6174087  0.94426596 0.21401747\n",
      "  0.11069497 0.9469942  0.48068184 0.9681034  0.41313925 0.29743963\n",
      "  0.8416083  0.6202419  0.5824169  0.22635405 0.11839538 0.2094927\n",
      "  0.9117539  0.5161936  0.10331562 0.54510826 0.5257167  0.16830607\n",
      "  0.59243035 0.6855068  0.07447074 0.11094531 0.7689338  0.44184098\n",
      "  0.3474238  0.15635541 0.16046679 0.278723   0.0932311  0.2217316\n",
      "  0.9279012  0.76593196 0.35014048 0.91539276]]\n",
      "tf.Tensor(\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], shape=(1, 40, 50000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(size=(1,40)).astype('float32')\n",
    "print(x)\n",
    "gpt2 = GPT2(12, 512, 8, 2048, 400, 50000)\n",
    "a, b = gpt2(x)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2(num_layers = num_layers, d_model=d_model, num_heads=num_heads, dff=dff, max_seq_len=max_seq_len, vocab_size=vocab_size)\n",
    "train_step_signature = [tf.TensorSpec(shape=(None, None), dtype=tf.int64)]\n",
    "\n",
    "# tf.function을 사용하면 그래프를 미리 컴파일 하기 때문에 속도가 상당히 빠름\n",
    "# 같은 GPU여도 케라스에 비해서 체감상 7~8배 정도의 차이가 나는 것 같음\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inputs, grad_clip=True, clip_value=2.5):\n",
    "    inp = inputs[:, :-1]\n",
    "    tar = inputs[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = gpt2(inp, True)\n",
    "        loss = loss_function(tar, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, gpt2.trainable_variables)  \n",
    "    if grad_clip:\n",
    "        gradients = [(tf.clip_by_value(grad, -clip_value, clip_value)) for grad in gradients]  \n",
    "    optimizer.apply_gradients(zip(gradients, gpt2.trainable_variables))\n",
    "  \n",
    "    accuracy = get_padded_accuracy(tar, predictions)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.6207 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 10.4015 Accuracy 0.0232\n",
      "Epoch 1 Batch 100 Loss 9.9555 Accuracy 0.0586\n",
      "Epoch 1 Batch 150 Loss 9.5209 Accuracy 0.0534\n",
      "Epoch 1 Batch 200 Loss 8.8616 Accuracy 0.0590\n",
      "Epoch 1 Batch 250 Loss 8.1687 Accuracy 0.0873\n",
      "Epoch 1 Batch 300 Loss 7.5581 Accuracy 0.1123\n",
      "Epoch 1 Batch 350 Loss 7.0891 Accuracy 0.1127\n",
      "Epoch 1 Batch 400 Loss 6.6717 Accuracy 0.1382\n",
      "Epoch 1 Batch 450 Loss 6.4759 Accuracy 0.1438\n",
      "Epoch 1 Batch 500 Loss 6.4461 Accuracy 0.1451\n",
      "Epoch 1 Batch 550 Loss 6.0227 Accuracy 0.1657\n",
      "Epoch 1 Batch 600 Loss 6.1645 Accuracy 0.1805\n",
      "Epoch 1 Batch 650 Loss 5.9188 Accuracy 0.1920\n",
      "Epoch 1 Batch 700 Loss 5.9567 Accuracy 0.1839\n",
      "Epoch 1 Batch 750 Loss 5.8723 Accuracy 0.2001\n",
      "Epoch 1 Loss 5.8872 Accuracy 0.1914\n",
      "Time taken for 1 epoch: 1210.4899151325226 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 5.6257 Accuracy 0.2032\n",
      "Epoch 2 Batch 50 Loss 5.5939 Accuracy 0.2080\n",
      "Epoch 2 Batch 100 Loss 5.6226 Accuracy 0.2097\n",
      "Epoch 2 Batch 150 Loss 5.6012 Accuracy 0.2241\n",
      "Epoch 2 Batch 200 Loss 5.7242 Accuracy 0.2154\n",
      "Epoch 2 Batch 250 Loss 5.5617 Accuracy 0.2117\n",
      "Epoch 2 Batch 300 Loss 5.4818 Accuracy 0.2306\n",
      "Epoch 2 Batch 350 Loss 5.4893 Accuracy 0.2253\n",
      "Epoch 2 Batch 400 Loss 5.2414 Accuracy 0.2447\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 20 에포크 훈련\n",
    "for epoch in range(20):\n",
    "    start = time.time()\n",
    "  \n",
    "  #train_loss.reset_states()\n",
    "  #train_accuracy.reset_states()\n",
    "  \n",
    "  # input : 포루투갈어, tar : 영어\n",
    "    for (batch, inp) in enumerate(dataset):\n",
    "        loss, accuracy = train_step(inp)\n",
    "    \n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, loss, accuracy))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                         loss,\n",
    "                                                         accuracy))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
